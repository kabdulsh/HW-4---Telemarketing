knitr::opts_chunk$set(echo = TRUE)
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
str(tele)
summary(tele)
#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL
# Deleting the column X
tele$X <- NULL
# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL
str(tele)
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric
#telemm <- as.data.frame(model.matrix(~.-1,tele))
#str(telemm)
#library(fastDummies)
#telemm <- dummy_cols(tele, remove_first_dummy = TRUE)
library(dummies)
telemm <- dummy.data.frame(tele[,-19])
# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random <- telemm[sample(nrow(telemm)),]
#Normalize the data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# we are going to normalize everything
tele_norm <- as.data.frame(lapply(tele_random, normalize))
# Selects 10000 random rows for test data
set.seed(12345)
test_set <- sample(1:nrow(tele_norm), 10000)
# Depending on R-version and computer, different rows may be selected.
# If that happens, results are different.
# Create a train set and test set
#First the predictors - all columns except the yyes column
tele_train <- tele_norm[-test_set, -match("yyes",names(tele_norm))]
tele_test <- tele_norm[test_set, -match("yyes",names(tele_norm))]
#Now the response (aka Labels) - only the yyes column
tele_train_labels <- tele_norm[-test_set, "yyes"]
tele_test_labels <- tele_norm[test_set, "yyes"]
# train the neuralnet model
library(neuralnet)
# simple ANN with only a single hidden neuron
tele_model <- neuralnet(yyes ~., data = tele_train)
# train the neuralnet model
library(neuralnet)
# simple ANN with only a single hidden neuron
tele_model <- neuralnet(yyes ~., tele_train)
# train the neuralnet model
library(neuralnet)
# simple ANN with only a single hidden neuron
tele_model <- neuralnet(yyes ~., data = tele_train)
# train the neuralnet model
library(neuralnet)
# simple ANN with only a single hidden neuron
tele_model <- neuralnet(formula = yyes ~., data = tele_train)
# train the neuralnet model
library(neuralnet)
# simple ANN with only a single hidden neuron
tele_model <- neuralnet(formula = yyes ~., data = tele_train)
knitr::opts_chunk$set(echo = TRUE)
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
str(tele)
knitr::opts_chunk$set(echo = TRUE)
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
str(tele)
summary(tele)
#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL
# Deleting the column X
tele$X <- NULL
# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL
str(tele)
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric
#telemm <- as.data.frame(model.matrix(~.-1,tele))
#str(telemm)
#library(fastDummies)
#telemm <- dummy_cols(tele, remove_first_dummy = TRUE)
library(dummies)
telemm <- dummy.data.frame(tele[,-19])
# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random <- telemm[sample(nrow(telemm)),]
#Normalize the data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
# we are going to normalize everything
tele_norm <- as.data.frame(lapply(tele_random, normalize))
# Selects 10000 random rows for test data
set.seed(12345)
test_set <- sample(1:nrow(tele_norm), 10000)
# Depending on R-version and computer, different rows may be selected.
# If that happens, results are different.
# Create a train set and test set
#First the predictors - all columns except the yyes column
tele_train <- tele_norm[-test_set, -match("yyes",names(tele_norm))]
tele_test <- tele_norm[test_set, -match("yyes",names(tele_norm))]
#Now the response (aka Labels) - only the yyes column
tele_train_labels <- tele_norm[-test_set, "yyes"]
tele_test_labels <- tele_norm[test_set, "yyes"]
# train the neuralnet model
library(neuralnet)
# simple ANN with only a single hidden neuron
tele_model <- neuralnet(formula = yyes ~., data = tele_train)
tele_train
test_set
# Create a train set and test set
#First the predictors - all columns except the yyes column
tele_train <- tele_norm[-test_set, -match("yyes",names(tele_norm))]
tele_train
tele_train <- tele_norm[-test_set,]
tele_test <- tele_norm[test_set,]
tele_train
telemm
head(tele[,-19])
head(tele[,])
y <- tele[19]
y
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
# Deleting the column X
tele$X <- NULL
#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL
# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL
str(tele)
classs
class(tele$y)
tele$y <- as.factor(tele$y)
class(tele$y)
library(caret)
library(e1071)
# Partition the data into training and testing data
set.seed(414)
index <- createDataPartition(tele$y, p = 0.7, list = FALSE)
training_set = tele[index,]
testing_set = tele[-index,]
# Set levels for both training and testing data
levels(training_set$y) <- make.names(levels(factor(training_set$y)))
levels(testing_set$y) <- make.names(levels(factor(testing_set$y)))
# Set up training controls
# Use repeated k-fold CV for resampling (i.e. three repeats of 10-fold CV)
# The results is 50 total resamples that are averaged
set.seed(414)
training_controls <- trainControl(method = "repeatedcv",
number = 10,
repeats = 5,
# Estimate class probabilities
classProbs = TRUE,
# Evaluate performance
summaryFunction = twoClassSummary)
knn_model <- train(y ~ ., data = training_set, method = "knn", preProcess("center", "scale"), trControl = training_control, metric = "ROC", tuneLength = 10)
# Set up training controls
# Use repeated k-fold CV for resampling (i.e. three repeats of 10-fold CV)
# The results is 30 total resamples that are averaged
set.seed(414)
training_control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
# Estimate class probabilities
classProbs = TRUE,
# Evaluate performance
summaryFunction = twoClassSummary)
knn_model <- train(y ~ ., data = training_set, method = "knn", preProcess("center", "scale"), trControl = training_control, metric = "ROC", tuneLength = 10)
str(training_set)
library(caret)
library(e1071)
# Transform the dependant variable into a factor, if not already
tele$y <- as.factor(tele$y)
telemm <- model.matrix(y ~ . -1, data = tele)
# Partition the data into training and testing data
set.seed(414)
index <- createDataPartition(telemm$y, p = 0.7, list = FALSE)
telemm
library(caret)
library(e1071)
# Transform the dependant variable into a factor, if not already
tele$y <- as.factor(tele$y)
# Partition the data into training and testing data
set.seed(414)
index <- createDataPartition(tele$y, p = 0.7, list = FALSE)
training_set = tele[index,]
testing_set = tele[-index,]
# Set levels for both training and testing data
levels(training_set$y) <- make.names(levels(factor(training_set$y)))
levels(testing_set$y) <- make.names(levels(factor(testing_set$y)))
# Set up training controls
# Use repeated k-fold CV for resampling (i.e. three repeats of 10-fold CV)
# The results is 30 total resamples that are averaged
set.seed(414)
training_control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 3,
# Estimate class probabilities
classProbs = TRUE,
# Evaluate performance
summaryFunction = twoClassSummary)
telemm <- model.matrix(y ~ . -1, data = tele)
knn_model <- train(y ~ ., data = telemm, method = "knn", preProcess("center", "scale"), trControl = training_control, metric = "ROC", tuneLength = 10)
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
# Deleting the column X
tele$X <- NULL
#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL
# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL
telemm <- model.matrix(y ~ . -1, data = tele)
library(caret)
library(e1071)
# Transform the dependant variable into a factor, if not already
telemm$y <- as.factor(telemm$y)
telemm
# Partition the data into training and testing data
set.seed(414)
index <- createDataPartition(telemm$y, p = 0.7, list = FALSE)
# Partition the data into training and testing data
set.seed(414)
index <- createDataPartition(y, p = 0.7, list = FALSE)
knitr::opts_chunk$set(echo = TRUE)
nn$result.matrix
plot(nn)
View(testset)
getwd()
setwd("~/Winter 2022/TO 414 001 WN 2022/HW #4 - Telemarketing")
head(tele_test)
tele_test <- tele_norm[test_set, # -match("yyes",names(tele_norm)
]
knitr::opts_chunk$set(echo = TRUE)
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL
# Deleting the column X
tele$X <- NULL
# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL
# Convert a factored, categorical variable to a numeric variable
tele[sapply(tele, is.factor)] <- data.matrix(tele[sapply(tele, is.factor)])
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric
# telemm <- as.data.frame(model.matrix(~.-1,tele))
# str(telemm)
# Randomize the rows in the data (shuffling the rows)
set.seed(414)
tele_random <- tele[sample(nrow(tele)),]
#Normalize the data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
library(dplyr)
tele_norm <- tele_random%>%mutate_if(is.numeric, normalize)
# we are going to normalize everything
#tele_norm <- as.data.frame(lapply(tele_random, normalize))
# Selects 10000 random rows for test data
set.seed(414)
test_set <- sample(1:nrow(tele_norm), 10000)
# Depending on R-version and computer, different rows may be selected.
# If that happens, results are different.
# Create a train set and test set
#First the predictors - all columns except the yyes column
tele_train <- tele_norm[-test_set,
# -match("yyes",names(tele_norm)
]
tele_test <- tele_norm[test_set, # -match("yyes",names(tele_norm)
]
#Now the response (aka Labels) - only the yyes column
# tele_train_labels <- tele_norm[-test_set, "yyes"]
# tele_test_labels <- tele_norm[test_set, "yyes"]
library(neuralnet)
nn <- neuralnet(y ~ ., data = tele_train, hidden = 1)
nn$result.matrix
plot(nn)
# Test the resulting output
# Remove dependent variables
temp_test <- subset(tele_test[-19])
head(temp_test)
nn_results <- compute(nn, temp_test)
results <- data.frame(actual = testset$left, prediction = nn_results$net.result)
# Test the resulting output
# Remove dependent variables
temp_test <- subset(tele_test[-19])
head(temp_test)
nn_results <- compute(nn, temp_test)
results <- data.frame(actual = tele_test$y, prediction = nn_results$net.result)
head(results)
# Confusion Matrix
library(caret)
rounded_results <- sapply(results, round, digits = 0)
rounded_results_df <- data.frame(rounded_results)
attach(rounded_results_df)
confusionMatrix(table(actual, prediction))
# Put dependent variable data in its own object
y <- tele["y"]
above.
y_train <- y[-test_set,]
y_test <- y[test_set,]
head(y_train)
y_train
y
tele_norm
# Everything up until now has been sufficient to run an artificial neural network (i.e. ANN)
# However, to run a kNN algorithm, we have to partition the data a bit differently.
# Put dependent variable data in its own object
y <- tele_norm["y"]
y
y_train <- y[-test_set,]
y_test <- y[test_set,]
y_train
kneighbors <- floor(sqrt(nrow(tele_norm)))
library(class)
tele_train
knn_pred <- knn(train = tele_train, test = tele_test, cl = y_train, k = kneighbors)
library(gmodels)
CrossTable(x = y_test, y = knn_pred, prop.chisq=FALSE)
confusionMatrix(as.factor(y_test), knn_pred)
knitr::opts_chunk$set(echo = TRUE)
summary(data)
temp_test
y
View(tele_norm)
tele_train_labels <- tele_norm[-test_set, 19]
tele_test_labels <- tele_norm[test_set, 19]
library(class)
library(gmodels)
# Run kNN Algorithm
kneighbors <- floor(sqrt(nrow(tele_norm)))
knn_pred <- knn(train = tele_train, test = tele_test, cl = tele_train_labels, k = kneighbors)
# Evaluate model performance
CrossTable(x = y_test, y = knn_pred, prop.chisq=FALSE)
confusionMatrix(as.factor(y_test), knn_pred)
kneighbors
library(class)
library(gmodels)
# Run kNN Algorithm
kneighbors <- 1
knn_pred <- knn(train = tele_train, test = tele_test, cl = tele_train_labels, k = kneighbors)
# Evaluate model performance
CrossTable(x = y_test, y = knn_pred, prop.chisq=FALSE)
confusionMatrix(as.factor(y_test), knn_pred)
library(class)
library(gmodels)
# Run kNN Algorithm
kneighbors <- 3
knn_pred <- knn(train = tele_train, test = tele_test, cl = tele_train_labels, k = kneighbors)
# Evaluate model performance
CrossTable(x = y_test, y = knn_pred, prop.chisq=FALSE)
confusionMatrix(as.factor(y_test), knn_pred)
# Run kNN Algorithm
kneighbors <- sqrt(nrow(tele_norm))
kneighbors
round(202.9482)
library(class)
library(gmodels)
# Run kNN Algorithm
kneighbors <- round(sqrt(nrow(tele_norm)))
knn_pred <- knn(train = tele_train, test = tele_test, cl = tele_train_labels, k = kneighbors)
# Evaluate model performance
CrossTable(x = y_test, y = knn_pred, prop.chisq=FALSE)
confusionMatrix(as.factor(y_test), knn_pred)
# Build a model with y-intercept only
# To be used in stepwise regression
FitStart <- glm(y ~ 1, data = tele_train, family = "binomial")
# Build a model with all variables
FitAll <- glm(y ~ ., data = tele_train, family = "binomial")
summary(FitAll)
# Use stepwise regression to find best model
step(FitStart, direction = "both", scope = formula(FitAll))
library(class)
library(gmodels)
# Run kNN Algorithm
kneighbors <- round(sqrt(nrow(tele_norm)))
knn_pred <- knn(train = tele_train, test = tele_test, cl = tele_train_labels, k = kneighbors)
# Evaluate model performance
CrossTable(x = tele_test_labels, y = knn_pred, prop.chisq=FALSE)
confusionMatrix(as.factor(tele_train_labels), knn_pred)
CrossTable(x = tele_test_labels, y = knn_pred, prop.chisq=FALSE)
confusionMatrix(tele_train_labels, knn_pred)
CrossTable(x = tele_test_labels, y = knn_pred, prop.chisq=FALSE)
confusionMatrix(tele_test_labels, knn_pred)
CrossTable(x = tele_test_labels, y = knn_pred, prop.chisq=FALSE)
confusionMatrix(as.factor(tele_test_labels), knn_pred)
lg <- glm(formula = y ~ nr.employed + pdaysdummy + month + poutcome +
contact + cons.conf.idx + default + campaign + euribor3m +
emp.var.rate + cons.price.idx + day_of_week + education +
marital + age, family = "binomial", data = tele_train)
results <- ifelse(predict(lg, newdata = tele_test, type = "response") > 0.5, 1 , 0)
CrossTable(x = tele_test$y, y = results, prop.chisq=FALSE)
confusionMatrix(tele_test$y, as.factor(results))
confusionMatrix(tele_test$y, results)
confusionMatrix(as.factor(tele_test$y), results)
tele_test$y
results
class(tele_test$y)
class(results)
confusionMatrix(as.factor(tele_test$y), as.factor(results))
lg <- glm(formula = y ~ nr.employed + pdaysdummy + month + poutcome +
contact + cons.conf.idx + default + campaign + euribor3m +
emp.var.rate + cons.price.idx + day_of_week + education +
marital + age, family = "binomial", data = tele_train)
results <- ifelse(predict(lg, newdata = tele_test, type = "response") > 0.6, 1 , 0)
CrossTable(x = tele_test$y, y = results, prop.chisq=FALSE)
confusionMatrix(as.factor(tele_test$y), as.factor(results))
lg <- glm(formula = y ~ nr.employed + pdaysdummy + month + poutcome +
contact + cons.conf.idx + default + campaign + euribor3m +
emp.var.rate + cons.price.idx + day_of_week + education +
marital + age, family = "binomial", data = tele_train)
results <- ifelse(predict(lg, newdata = tele_test, type = "response") > 0.5, 1 , 0)
CrossTable(x = tele_test$y, y = results, prop.chisq=FALSE)
confusionMatrix(as.factor(tele_test$y), as.factor(results))
rounded_results
rounded_results_df
prediction
# Make a copy of tele_test to avoid any problems.
copy_tele_test <- tele_test
# Predicted majority
copy_tele_test$pred_majority <- as.factor(ifelse(prediction==1 & knn_pred==1,1,ifelse(prediction==1 & results==1,1,ifelse(knn_pred==1 & results==1,1,0))))
copy_tele_test
tele
# Make a copy of tele_test to avoid any problems.
copy_tele <- tele
# Predicted majority
copy_tele$pred_majority <- as.factor(ifelse(prediction==1 & knn_pred==1,1,ifelse(prediction==1 & results==1,1,ifelse(knn_pred==1 & results==1,1,0))))
# Make a copy of tele_test to avoid any problems.
copy_tele_test <- tele_test
# Predicted majority
copy_tele_test$pred_majority <- as.factor(ifelse(prediction==1 & knn_pred==1,1,ifelse(prediction==1 & results==1,1,ifelse(knn_pred==1 & results==1,1,0))))
copy_tele_test
comparison <- subset(copy_tele_test, "y", "pred_majority")
comparison <- subset(copy_tele_test, y, pred_majority)
comparison <- subset(copy_tele_test["y", "pred_majority"])
comparison <- subset(copy_tele_test, y, pred_majority)
class(copy_tele_test)
comparison <- copy_tele_test[ , c("y", "pred_majority")]
comparison
confusionMatrix(comparison$y, comparison$pred_majority)
confusionMatrix(as.factor(comparison$y), as.factor(comparison$pred_majority))
confusionMatrix(as.factor(copy_tele_test$y), as.factor(copy_tele_test$pred_majority))
knitr::opts_chunk$set(echo = TRUE)
confusionMatrix(table(actual, prediction))
getwd()
setwd("~/Winter 2022/TO 414 001 WN 2022/HW #4 - Telemarketing")
knitr::opts_chunk$set(echo = TRUE)
# Download & prepare data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
# Delete "duration" variable b/c it's an after the fact measurement.
# Only use variables known before call
tele$duration <- NULL
# Delete column X
tele$X <- NULL
# Change pdays to a dummy variable, and delete pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL
# Convert a factored, categorical variable to a numeric variable
tele[sapply(tele, is.factor)] <- data.matrix(tele[sapply(tele, is.factor)])
# Randomize rows in data
set.seed(414)
tele_random <- tele[sample(nrow(tele)),]
# Normalize data
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
library(dplyr)
tele_norm <- tele_random%>%mutate_if(is.numeric, normalize)
# Select 10,000 random rows for test data
set.seed(414)
test_set <- sample(1:nrow(tele_norm), 10000)
# Create train set & test set
tele_train <- tele_norm[-test_set,]
tele_test <- tele_norm[test_set,]
# Create separate data frame for 'y' feature, which is our target
tele_train_labels <- tele_norm[-test_set, 19]
tele_test_labels <- tele_norm[test_set, 19]
library(neuralnet)
nn <- neuralnet(y ~ ., data = tele_train, hidden = 1)
